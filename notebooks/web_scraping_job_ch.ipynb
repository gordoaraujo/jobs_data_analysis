{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('../src')))\n",
    "from src.getjobsch import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Jobs.ch\n",
    "\n",
    "As a job seeker, one has to search through job portals to find most relevant jobs related to your profile. In this exercise, your goal is to find all jobs related to keywords: “Data Scientist”, “Data Analyst”, “Python Developer”, “Data Engineer”, “Data Manager”, “Data Architect”, “Big Data Analyst” and “Data Python” on jobs.ch.\n",
    "1. Download all necessary information (including job title, date, company name, location…) for all webpages.\n",
    "2. Using the information obtained, perform a descriptive analysis on this data including questions:\n",
    "   - How many jobs are shared between these categories?\n",
    "   - How much the keywords: “Data Analyst” and “Big Data Analyst” overlap?\n",
    "   - Are there some companies doing more hires than average?\n",
    "   - How many jobs are there in different Kantons?\n",
    "   - Is “machine learning” keyword more often in data scientist or data analyst jobs?\n",
    "   - What is the distribution of most common keywords between and across categories?\n",
    "3. Produce a report in the form of a clean notebook (or jupyter slides), with commented code and markdown cells for structuring and interpretations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "The file `src/getjobsch` contains the necesary functions to pull infomation from https://www.jobs.ch/en/vacancies/. The function works in the following way:\n",
    "- Receives a list of job positions on natural language\n",
    "- The function `clean_job_keywords` will transform those key words to search keywords by removing white spaces and replacing them with `%20` characters\n",
    "- Once the necesary keywords were obtained the function `df_full_data` will proceed to pull info for each job in the following way:\n",
    "  - Get the number of available pages for each job position\n",
    "  - For each of the available pages, scrap an individual text box using the function `get_data_one_job` and concatenating the info by using the function `df_all_jobs`\n",
    "  - In case no job postings are found an error should be printed (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key words to be searched\n",
    "job_positions = [\"Data Engineer\", \"Data Scientist\", \"Data Analyst\", \"Python Developer\", \"Data Manager\", \"Data Architect\", \"Big Data Analyst\", \"Data Python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the function to get both errors and \n",
    "# df_all = df_full_data(job_positions)\n",
    "\n",
    "# # In this case we should not have errors\n",
    "# errors = df_all[\"errors\"]\n",
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the found jobs\n",
    "# df_jobs = df_all[\"results\"]\n",
    "# df_jobs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load found jobs\n",
    "df_jobs = pd.read_csv(\"../data/raw/df_jobs_ch.csv\", index_col=[0])\n",
    "df_jobs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an index problem for some cases and therefore some job types do not make sense\n",
    "df_jobs.job_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_summary, skills_summary, python_summary, errors = get_job_keywords(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There were {len(errors['errors'])} positions without available information\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs.to_csv(\"../data/raw/df_jobs_ch.csv\")\n",
    "# pd.DataFrame(dict(programming_summary).items()).to_csv(\"../data/raw/programming_summary.csv\")\n",
    "# pd.DataFrame(dict(python_summary).items()).to_csv(\"../data/raw/python_summary.csv\")\n",
    "# pd.DataFrame(dict(skills_summary).items()).to_csv(\"../data/raw/skills_summary.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping -- Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_urls = df_jobs[\"job_link\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_count = []\n",
    "skills_count = []\n",
    "python_count = []\n",
    "errors = []\n",
    "\n",
    "for i, ju in enumerate(job_urls):\n",
    "    flag_1 = False\n",
    "    flag_2 = False\n",
    "    sec_page = requests.get(ju)\n",
    "    sec_soup = BeautifulSoup(sec_page.content, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        job_desc = sec_soup.find(\"div\", {\"data-cy\" : \"vacancy-description\"})\n",
    "        job_desc_text = job_desc.text\n",
    "    except AttributeError:\n",
    "        job_desc_text = \"\"\n",
    "        flag_1 = True\n",
    "    \n",
    "    if job_desc == None:\n",
    "        try:\n",
    "            job_desc = sec_soup.find(\"iframe\", {\"data-cy\" : \"detail-vacancy-iframe-content\"}).find_next()\n",
    "            job_desc_text = job_desc.text\n",
    "        except AttributeError:\n",
    "            job_desc_text = \"\"\n",
    "            flag_2 = True\n",
    "    \n",
    "    if flag_1 and flag_2:\n",
    "        errors.append(ju)\n",
    "\n",
    "    job_desc_text = job_desc_text.translate(job_desc_text.maketrans(\"\", \"\", '!\"$%&\\'()*,-./:;<=>?@[\\\\]^_`{|}~'))\n",
    "    job_desc_text = job_desc_text.lower()\n",
    "    job_desc_words = job_desc_text.split()\n",
    "    \n",
    "    for word in job_desc_words:\n",
    "        if word in keywords_programming.keys():\n",
    "            programming_count.append(word) #  = word_count.get(word, 0) + 1\n",
    "        if word in keywords_skills.keys():\n",
    "            skills_count.append(word)\n",
    "        if word in keywords_python.keys():\n",
    "            python_count.append(word)\n",
    "    \n",
    "    programming_summary = collections.Counter(programming_count)\n",
    "    skills_summary = collections.Counter(skills_count)\n",
    "    python_summary = collections.Counter(python_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(programming_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skills_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping -- Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_url = df_jobs[[\"job_link\"]].values[1][0] # 'https://www.jobs.ch/en/vacancies/detail/3fa23bd2-215b-4500-83cf-317d15b71d13/?source=vacancy_search'\n",
    "sec_page = requests.get(sec_url)\n",
    "sec_soup = BeautifulSoup(sec_page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_soup.find(\"div\", {\"data-cy\" : \"vacancy-description\"}).text.find(\"Git\") > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_soup.find(\"div\", {\"data-cy\" : \"vacancy-description\"}).text[936:939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = sec_soup.find(\"div\", {\"data-cy\" : \"vacancy-description\"}).find_all(\"ul\", {\"class\" : \"Ul-sc-1n42qu0-0 Ul-sc-1otw97l-0 JJPIu kNGQob\"}) # .find_all(\"span\")\n",
    "for l in ll:\n",
    "    l.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_soup.find(\"iframe\", {\"data-cy\" : \"detail-vacancy-iframe-content\"}).find_next().text.find(\"Python\") # find_all(\"p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
