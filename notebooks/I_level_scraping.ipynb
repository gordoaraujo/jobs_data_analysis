{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2f497a-4c96-4fed-8d16-b22feec594ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87f2481-2153-4be0-b62b-8a6cf4ca304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in lst:\n",
    "            file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff78c649-60ba-4c1e-bba0-06b04538f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs = [\n",
    "    \"data scientist\",\n",
    "    \"data science\",\n",
    "    \"data engineer\",\n",
    "    #\"data analyst\",\n",
    "#    \"data architect\",\n",
    " #   \"data consultant\",\n",
    "  #  \"data manager\",\n",
    "   # \"data operations\",\n",
    "#    \"data governance\",\n",
    " #   \"data quality analyst\",\n",
    "    \"data visualization specialist\",\n",
    "  #  \"data modeler\",\n",
    "    #\"data strategist\",\n",
    "    \"data warehouse developer\",\n",
    "    #\"data entry specialist\",\n",
    "    \"data research analyst\",\n",
    "    #\"data privacy officer\",\n",
    "    \"data migration specialist\",\n",
    "    #\"data analytics manager\",\n",
    "    \"data integration engineer\",\n",
    "    \"data solutions architect\",\n",
    "    \"data mining specialist\",\n",
    "   # \"data product manager\",\n",
    "    #\"data governance analyst\",\n",
    "    \"data security engineer\",\n",
    "    #\"data validation analyst\",\n",
    "    #\"data reporting analyst\",\n",
    "    #\"data systems analyst\",\n",
    "    \"data optimization engineer\",\n",
    "    \"data transformation analyst\",\n",
    "    #\"data analytics consultant\",\n",
    "    #\"data operations manager\",\n",
    "    \"data platform engineer\",\n",
    "    \"data visualization engineer\",\n",
    "    \"data strategy consultant\",\n",
    "    \"data warehouse architect\",\n",
    "    #\"data analyst intern\",\n",
    "    #\"data analytics specialist\",\n",
    "    \"data infrastructure engineer\",\n",
    "    \"data management analyst\",\n",
    "    \"data insights analyst\",\n",
    "    \"data governance manager\",\n",
    "    \"data warehouse specialist\",\n",
    "    \"data analytics engineer\",\n",
    "    #\"data privacy analyst\",\n",
    "    \"data migration analyst\",\n",
    "    \"data scientist intern\",\n",
    "    \"machine learning\",\n",
    "    \"NLP\",\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"natural language process\",\n",
    "    \"neural network\",\n",
    "    \"SQL\"\n",
    "]\n",
    "data_jobs = [job_title for job_title in data_jobs if \" \".join(job_title.split()[:-1]) not in data_jobs]\n",
    "\n",
    "data_jobs = [\n",
    "    \"-\".join(job_title.split()) for job_title in data_jobs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd6e274-6466-4337-a56e-a82ca5196861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: 1\n",
      "Scraping page: 2\n",
      "\u001b[H\u001b[2JFound 1\n",
      "Scraping page: 3\n",
      "\u001b[H\u001b[2JFound 2\n",
      "Scraping page: 4\n",
      "\u001b[H\u001b[2JFound 3\n",
      "\u001b[H\u001b[2JFound 4\n",
      "Scraping page: 5\n",
      "\u001b[H\u001b[2JFound 5\n",
      "\u001b[H\u001b[2JFound 6\n",
      "\u001b[H\u001b[2JFound 7\n",
      "\u001b[H\u001b[2JFound 8\n",
      "\u001b[H\u001b[2JFound 9\n",
      "Scraping page: 6\n",
      "\u001b[H\u001b[2JFound 10\n",
      "Scraping page: 7\n",
      "\u001b[H\u001b[2JFound 11\n",
      "Scraping page: 8\n",
      "\u001b[H\u001b[2JFound 12\n",
      "\u001b[H\u001b[2JFound 13\n",
      "Scraping page: 9\n",
      "\u001b[H\u001b[2JFound 14\n",
      "Scraping page: 10\n"
     ]
    }
   ],
   "source": [
    "job_postings= []\n",
    "for i in range(1,11):\n",
    "    page = requests.get(f\"https://www.governmentjobs.com/jobs?page={i}&keyword=data&isTransfer=False&isPromotional=False\")\n",
    "    print(f\"Scraping page: {i}\")\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\") \n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if any(keyword.lower() in link['href'].lower() for keyword in data_jobs):        \n",
    "        #if 'data' in link['href']:\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            job_postings.append(link['href'])\n",
    "            print(f\"Found {len(job_postings)}\")\n",
    "\n",
    "job_links = [\"https://www.governmentjobs.com\"+ item for item in job_postings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e127a4-ec14-4ea4-9117-5446fecbab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'scraped_jobs.txt'\n",
    "save_list_to_file(job_links, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee677d9d-0baf-4610-96cb-9ed754ab16d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_toolkit",
   "language": "python",
   "name": "ds_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
